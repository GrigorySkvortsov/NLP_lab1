# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫
import streamlit as st
import json
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from collections import Counter
from nltk.tokenize import word_tokenize
from razdel import tokenize as razdel_tokenize
from nltk.stem import SnowballStemmer, PorterStemmer
import nltk
import spacy
import subprocess
import pdfkit
import os
from io import StringIO

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤
def ensure_nltk_resources():
    try:
        nltk.data.find("tokenizers/punkt")
    except LookupError:
        nltk.download("punkt")

# –∑–∞–≥—Ä—É–∑–∫–∞ spaCy –º–æ–¥–µ–ª–µ–π
def load_spacy_model(lang):
    try:
        if lang == "–†—É—Å—Å–∫–∏–π":
            return spacy.load("ru_core_news_sm")
        else:
            return spacy.load("en_core_web_sm")
    except OSError:
        st.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å spaCy –¥–ª—è —è–∑—ã–∫–∞ {lang} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ—ë –≤—Ä—É—á–Ω—É—é.")
        return None

# –ß—Ç–µ–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞
def process_corpus(input_file="3_news_corpus_universal.jsonl"): 
    texts = []
    with open(input_file, "r", encoding="utf-8") as f:
        for line in f:
            try:
                article = json.loads(line.strip())
                text = article.get("preprocessed_text", article.get("cleaned_text", article.get("text", "")))
                if text:
                    texts.append(text)
            except:
                continue
    return texts

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
def naive_tokenize(text):
    return text.split()

def regex_tokenize(text):
    import re
    return re.findall(r"\b\w+\b", text)

def nltk_tokenize(text, lang):
    lang = "russian" if lang == "–†—É—Å—Å–∫–∏–π" else "english"
    return word_tokenize(text, language=lang)

def razdel_tok(text):
    return [t.text for t in razdel_tokenize(text)]

def spacy_tokenize(text, nlp):
    if nlp is None:
        return []
    doc = nlp(text)
    return [t.text for t in doc if t.text.strip()]

def porter_stem(tokens):
    stemmer = PorterStemmer()
    return [stemmer.stem(t) for t in tokens]

def snowball_stem(tokens, lang):
    lang = "russian" if lang == "–†—É—Å—Å–∫–∏–π" else "english"
    stemmer = SnowballStemmer(lang)
    return [stemmer.stem(t) for t in tokens]

def pymorphy_lemmatize(tokens):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è pymorphy2 —Å —Ñ–∏–∫—Å–æ–º –¥–ª—è Python 3.13"""
    import inspect
    if not hasattr(inspect, "getargspec"):
        from collections import namedtuple
        ArgSpec = namedtuple("ArgSpec", "args varargs keywords defaults")
        def getargspec(func):
            spec = inspect.getfullargspec(func)
            return ArgSpec(spec.args, spec.varargs, spec.varkw, spec.defaults)
        inspect.getargspec = getargspec

    try:
        from pymorphy2 import MorphAnalyzer
        morph = MorphAnalyzer()
        print("pymorphy2 —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        return [morph.parse(t)[0].normal_form for t in tokens]
    except Exception as e:
        print("–û—à–∏–±–∫–∞ pymorphy2:", e)
        return None

def spacy_lemmatize(tokens, nlp):
    if nlp is None:
        return tokens
    doc = nlp(" ".join(tokens))
    return [t.lemma_ for t in doc]

# –ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ç—á—ë—Ç—ã
def compute_metrics(tokens_list, vocab):
    token_lengths = [len(t) for toks in tokens_list for t in toks]
    total_tokens = len(token_lengths)
    oov_tokens = sum(1 for toks in tokens_list for t in toks if t not in vocab)
    oov_percentage = oov_tokens / total_tokens * 100 if total_tokens > 0 else 0

    freq = Counter(t for toks in tokens_list for t in toks)
    top_tokens = dict(sorted(freq.items(), key=lambda x: x[1], reverse=True)[:10])

    return {
        "token_lengths": token_lengths,
        "oov_percentage": oov_percentage,
        "token_freq": top_tokens,
        "vocab_size": len(vocab)
    }

def generate_report(metrics, tokenizer, normalizer, lang):
    report = f"# –û—Ç—á—ë—Ç –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞\n\n"
    report += f"**–Ø–∑—ã–∫:** {lang}\n"
    report += f"**–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è:** {tokenizer}\n"
    report += f"**–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è:** {normalizer}\n\n"
    report += f"**–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è:** {metrics['vocab_size']}\n"
    report += f"**–î–æ–ª—è OOV:** {metrics['oov_percentage']:.2f}%\n\n"
    report += "## –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ (—Ç–æ–ø-10)\n"
    report += "| –¢–æ–∫–µ–Ω | –ß–∞—Å—Ç–æ—Ç–∞ |\n|--------|---------|\n"
    for token, freq in metrics["token_freq"].items():
        report += f"| {token} | {freq} |\n"
    return report

# –û—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ Streamlit
def main():
    st.title("üß† –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞")

    ensure_nltk_resources()

    # –≤—ã–±–æ—Ä —è–∑—ã–∫–∞
    lang = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞", ["–†—É—Å—Å–∫–∏–π", "–ê–Ω–≥–ª–∏–π—Å–∫–∏–π"])

    # –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ä–ø—É—Å–∞
    st.subheader("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ä–ø—É—Å–∞")
    use_default = st.checkbox("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å")
    uploaded = st.file_uploader("–ò–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Å–≤–æ–π .jsonl —Ñ–∞–π–ª", type=["jsonl"])

    if use_default:
        corpus_path = "3_news_corpus_universal.jsonl"
    elif uploaded:
        corpus_path = "uploaded_corpus.jsonl"
        with open(corpus_path, "wb") as f:
            f.write(uploaded.getbuffer())
    else:
        st.stop()

    texts = process_corpus(corpus_path)
    if not texts:
        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–æ—Ä–ø—É—Å.")
        st.stop()
    st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")

    # –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ spaCy
    nlp = load_spacy_model(lang)

    # –≤—ã–±–æ—Ä –º–µ—Ç–æ–¥–æ–≤
    st.subheader("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    tokenizer = st.selectbox("–ú–µ—Ç–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏", ["–ù–∞–∏–≤–Ω–∞—è", "–†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è", "NLTK", "razdel", "spaCy"])
    normalizer = st.selectbox("–ú–µ—Ç–æ–¥ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏", ["–ë–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏", "PorterStemmer", "SnowballStemmer", "pymorphy2", "spaCy Lemmatizer"])

    # –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ—Ä–ø—É—Å–∞
    if st.button("üîç –í—ã–ø–æ–ª–Ω–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É"):
        tokens_list = []
        vocab = set()

        progress = st.progress(0)
        for i, text in enumerate(texts):
            # —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            if tokenizer == "–ù–∞–∏–≤–Ω–∞—è":
                tokens = naive_tokenize(text)
            elif tokenizer == "–†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è":
                tokens = regex_tokenize(text)
            elif tokenizer == "NLTK":
                tokens = nltk_tokenize(text, lang)
            elif tokenizer == "razdel":
                tokens = razdel_tok(text)
            elif tokenizer == "spaCy":
                tokens = spacy_tokenize(text, nlp)
            else:
                tokens = []

            # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            if normalizer == "PorterStemmer":
                tokens = porter_stem(tokens)
            elif normalizer == "SnowballStemmer":
                tokens = snowball_stem(tokens, lang)
            elif normalizer == "pymorphy2":
                tokens = pymorphy_lemmatize(tokens)
            elif normalizer == "spaCy Lemmatizer":
                tokens = spacy_lemmatize(tokens, nlp)

            if tokens:
                tokens_list.append(tokens)
                vocab.update(tokens)
            progress.progress((i + 1) / len(texts))

        # –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        metrics = compute_metrics(tokens_list, vocab)

        # –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è")

        fig1 = px.histogram(metrics["token_lengths"], nbins=50, title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω —Ç–æ–∫–µ–Ω–æ–≤")
        st.plotly_chart(fig1)
        fig1.write_image("token_lengths.png")

        freq_df = pd.DataFrame(metrics["token_freq"].items(), columns=["–¢–æ–∫–µ–Ω", "–ß–∞—Å—Ç–æ—Ç–∞"])
        fig2 = px.bar(freq_df, x="–¢–æ–∫–µ–Ω", y="–ß–∞—Å—Ç–æ—Ç–∞", title="–ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ (—Ç–æ–ø-10)")
        st.plotly_chart(fig2)

        fig3 = go.Figure()
        fig3.add_trace(go.Bar(name="OOV", x=["OOV"], y=[metrics["oov_percentage"]]))
        fig3.add_trace(go.Bar(name="In-Vocab", x=["In-Vocab"], y=[100 - metrics["oov_percentage"]]))
        fig3.update_layout(title="–î–æ–ª—è OOV", barmode="stack")
        st.plotly_chart(fig3)

        # –æ—Ç—á—ë—Ç
        report = generate_report(metrics, tokenizer, normalizer, lang)
        st.markdown(report)

        # —ç–∫—Å–ø–æ—Ä—Ç
        st.subheader("üì§ –≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á—ë—Ç–∞")
        with open("report.html", "w", encoding="utf-8") as f:
            f.write(f"<html><body>{report}</body></html>")

        with open("report.html", "r", encoding="utf-8") as f:
            st.download_button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å HTML", f, file_name="report.html")

        with open("report.html", "r", encoding="utf-8") as f:
            st.download_button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å pdf", f, file_name="report.html")

        # try:
        #     path_wkhtmltopdf = r"C:\Program Files\wkhtmltopdf\bin\wkhtmltopdf.exe"
        #     config = pdfkit.configuration(wkhtmltopdf=path_wkhtmltopdf)
        #     pdfkit.from_file("report.html", "report.pdf", configuration=config)
        #     with open("report.pdf", "rb") as f:
        #         st.download_button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å PDF", f, file_name="report.pdf", mime="application/pdf")
        # except Exception as e:
        #     st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å PDF: {str(e)}")


# =====================[ –ó–∞–ø—É—Å–∫ ]=====================
if __name__ == "__main__":
    main()
