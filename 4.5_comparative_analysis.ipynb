{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\VSCode\\NLP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ú–æ–¥–µ–ª—å spaCy ru_core_news_sm –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\VSCode\\NLP\\.venv\\Lib\\site-packages\\pymorphy2\\analyzer.py:114: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ pymorphy2 —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ –º–æ–¥–µ–ª–µ–π\n",
    "\n",
    "def ensure_spacy_model():\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–ª–∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç ru_core_news_sm\n",
    "    try:\n",
    "        nlp = spacy.load(\"ru_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
    "        print(\"–ú–æ–¥–µ–ª—å spaCy ru_core_news_sm –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
    "        return nlp\n",
    "    except OSError:\n",
    "        print(\"–ú–æ–¥–µ–ª—å ru_core_news_sm –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é...\")\n",
    "        try:\n",
    "            subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"ru_core_news_sm\"], check=True)\n",
    "            nlp = spacy.load(\"ru_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
    "            print(\"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞\")\n",
    "            return nlp\n",
    "        except Exception as e:\n",
    "            print(\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ spaCy:\", e)\n",
    "            return None\n",
    "\n",
    "\n",
    "def ensure_pymorphy():\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è pymorphy2 —Å —Ñ–∏–∫—Å–æ–º –¥–ª—è Python 3.13\n",
    "    import inspect\n",
    "    if not hasattr(inspect, \"getargspec\"):\n",
    "        from collections import namedtuple\n",
    "        ArgSpec = namedtuple(\"ArgSpec\", \"args varargs keywords defaults\")\n",
    "        def getargspec(func):\n",
    "            spec = inspect.getfullargspec(func)\n",
    "            return ArgSpec(spec.args, spec.varargs, spec.varkw, spec.defaults)\n",
    "        inspect.getargspec = getargspec\n",
    "\n",
    "    try:\n",
    "        from pymorphy2 import MorphAnalyzer\n",
    "        morph = MorphAnalyzer()\n",
    "        print(\"pymorphy2 —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\")\n",
    "        return morph\n",
    "    except Exception as e:\n",
    "        print(\"–û—à–∏–±–∫–∞ pymorphy2:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def ensure_nltk_resources():\n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –Ω—É–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ NLTK\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/punkt_tab\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"punkt_tab\")\n",
    "\n",
    "ensure_nltk_resources()\n",
    "spacy_nlp = ensure_spacy_model()\n",
    "morph = ensure_pymorphy()\n",
    "sentence_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(\"russian\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ú–µ—Ç–æ–¥—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "\n",
    "def naive_tokenize(text):\n",
    "    return [t for t in text.split() if t.strip()]\n",
    "\n",
    "def regex_tokenize(text):\n",
    "    return [t for t in re.findall(r\"\\b\\w+\\b\", text.lower()) if t.strip()]\n",
    "\n",
    "def nltk_tokenize(text):\n",
    "    try:\n",
    "        return [t for t in word_tokenize(text, language=\"russian\") if t.strip()]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def razdel_tokenize_text(text):\n",
    "    return [t.text for t in razdel_tokenize(text) if t.text.strip()]\n",
    "\n",
    "def spacy_tokenize(text):\n",
    "    if spacy_nlp is None:\n",
    "        return []\n",
    "    doc = spacy_nlp(text)\n",
    "    return [token.text for token in doc if token.text.strip()]\n",
    "\n",
    "# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "\n",
    "def porter_stem(tokens):\n",
    "    return [porter.stem(t) for t in tokens]\n",
    "\n",
    "def snowball_stem(tokens):\n",
    "    return [snowball.stem(t) for t in tokens]\n",
    "\n",
    "def pymorphy_lemmatize(tokens):\n",
    "    if morph is None:\n",
    "        return tokens\n",
    "    return [morph.parse(t)[0].normal_form for t in tokens]\n",
    "\n",
    "def spacy_lemmatize(tokens):\n",
    "    if spacy_nlp is None:\n",
    "        return tokens\n",
    "    doc = spacy_nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ú–µ—Ç—Ä–∏–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏\n",
    "\n",
    "def compute_oov(tokens_list, vocab):\n",
    "    total_tokens = sum(len(toks) for toks in tokens_list)\n",
    "    oov = sum(1 for toks in tokens_list for t in toks if t not in vocab)\n",
    "    return oov / total_tokens * 100 if total_tokens > 0 else 0\n",
    "\n",
    "def compute_cosine_similarity(original_text, processed_tokens):\n",
    "    processed_text = \" \".join(processed_tokens)\n",
    "    if not processed_text or not original_text:\n",
    "        return 0.0\n",
    "    emb = sentence_model.encode([original_text, processed_text], convert_to_tensor=True)\n",
    "    return util.cos_sim(emb[0], emb[1]).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞\n",
    "\n",
    "def process_corpus(input_file=\"3_news_corpus_universal.jsonl\"):\n",
    "    texts = []\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                article = json.loads(line.strip())\n",
    "                text = article.get(\"preprocessed_text\", article.get(\"cleaned_text\", article.get(\"text\", \"\")))\n",
    "                if text:\n",
    "                    texts.append(text)\n",
    "            except:\n",
    "                continue\n",
    "    return texts\n",
    "\n",
    "\n",
    "def run_experiment(texts):\n",
    "    methods = [\n",
    "        (\"naive\", naive_tokenize, None),\n",
    "        (\"regex\", regex_tokenize, None),\n",
    "        (\"nltk\", nltk_tokenize, None),\n",
    "        (\"razdel\", razdel_tokenize_text, None),\n",
    "        (\"nltk_porter\", nltk_tokenize, porter_stem),\n",
    "        (\"nltk_snowball\", nltk_tokenize, snowball_stem)\n",
    "    ]\n",
    "    if spacy_nlp:\n",
    "        methods.extend([\n",
    "            (\"spacy\", spacy_tokenize, None),\n",
    "            (\"spacy_lem\", spacy_tokenize, spacy_lemmatize)\n",
    "        ])\n",
    "    if morph:\n",
    "        methods.append((\"nltk_pymorphy\", nltk_tokenize, pymorphy_lemmatize))\n",
    "\n",
    "    results, vocab = [], set()\n",
    "\n",
    "    for name, tokenize_fn, normalize_fn in methods:\n",
    "        print(f\"\\n–ú–µ—Ç–æ–¥: {name}\")\n",
    "        start = time.time()\n",
    "        tokens_list, similarities = [], []\n",
    "\n",
    "        for text in texts:\n",
    "            tokens = tokenize_fn(text)\n",
    "            if normalize_fn:\n",
    "                tokens = normalize_fn(tokens)\n",
    "            tokens_list.append(tokens)\n",
    "            if len(similarities) < 10:\n",
    "                sim = compute_cosine_similarity(text, tokens)\n",
    "                similarities.append(sim)\n",
    "\n",
    "        vocab_size = len({t for toks in tokens_list for t in toks})\n",
    "        vocab.update(t for toks in tokens_list for t in toks)\n",
    "\n",
    "        results.append({\n",
    "            \"method\": name,\n",
    "            \"vocab_size\": vocab_size,\n",
    "            \"avg_similarity\": sum(similarities)/len(similarities) if similarities else 0.0,\n",
    "            \"time_per_1000\": (time.time() - start) / len(texts) * 1000,\n",
    "            \"oov_percentage\": compute_oov(tokens_list, vocab)\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "\n",
    "def save_results(results):\n",
    "    pd.DataFrame(results).to_csv(\"tokenization_metrics.csv\", index=False)\n",
    "    print(\"–ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ tokenization_metrics.csv\")\n",
    "\n",
    "def save_report(results):\n",
    "    md = \"# –û—Ç—á—ë—Ç –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é –º–µ—Ç–æ–¥–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏\\n\\n\"\n",
    "    md += \"| –ú–µ—Ç–æ–¥ | –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è | OOV (%) | –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ | –í—Ä–µ–º—è –Ω–∞ 1000 —Å—Ç–∞—Ç–µ–π (—Å) |\\n\"\n",
    "    md += \"|-------|----------------|---------|---------------------|--------------------------|\\n\"\n",
    "    for r in results:\n",
    "        md += f\"| {r['method']} | {r['vocab_size']} | {r['oov_percentage']:.2f} | {r['avg_similarity']:.4f} | {r['time_per_1000']:.2f} |\\n\"\n",
    "    with open(\"analysis_report.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(md)\n",
    "    print(\"üìò –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ analysis_report.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∂–µ–Ω–æ 1000 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
      "\n",
      "‚ñ∂Ô∏è –ú–µ—Ç–æ–¥: naive\n",
      "\n",
      "‚ñ∂Ô∏è –ú–µ—Ç–æ–¥: regex\n",
      "\n",
      "‚ñ∂Ô∏è –ú–µ—Ç–æ–¥: nltk\n",
      "\n",
      "‚ñ∂Ô∏è –ú–µ—Ç–æ–¥: razdel\n",
      "\n",
      "‚ñ∂Ô∏è –ú–µ—Ç–æ–¥: nltk_porter\n",
      "\n",
      "‚ñ∂Ô∏è –ú–µ—Ç–æ–¥: nltk_snowball\n",
      "\n",
      "‚ñ∂Ô∏è –ú–µ—Ç–æ–¥: spacy\n",
      "\n",
      "‚ñ∂Ô∏è –ú–µ—Ç–æ–¥: spacy_lem\n",
      "\n",
      "‚ñ∂Ô∏è –ú–µ—Ç–æ–¥: nltk_pymorphy\n",
      "üìÅ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ tokenization_metrics.csv\n",
      "üìò –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ analysis_report.md\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>avg_similarity</th>\n",
       "      <th>time_per_1000</th>\n",
       "      <th>oov_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naive</td>\n",
       "      <td>44333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.848534</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>regex</td>\n",
       "      <td>34259</td>\n",
       "      <td>0.917866</td>\n",
       "      <td>1.177768</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nltk</td>\n",
       "      <td>34769</td>\n",
       "      <td>0.971512</td>\n",
       "      <td>2.211550</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>razdel</td>\n",
       "      <td>34478</td>\n",
       "      <td>0.971297</td>\n",
       "      <td>1.786527</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nltk_porter</td>\n",
       "      <td>34750</td>\n",
       "      <td>0.971005</td>\n",
       "      <td>3.552431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nltk_snowball</td>\n",
       "      <td>17434</td>\n",
       "      <td>0.859068</td>\n",
       "      <td>5.028917</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spacy</td>\n",
       "      <td>34695</td>\n",
       "      <td>0.970112</td>\n",
       "      <td>50.009183</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spacy_lem</td>\n",
       "      <td>19468</td>\n",
       "      <td>0.924172</td>\n",
       "      <td>97.268040</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nltk_pymorphy</td>\n",
       "      <td>17603</td>\n",
       "      <td>0.929640</td>\n",
       "      <td>18.611283</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          method  vocab_size  avg_similarity  time_per_1000  oov_percentage\n",
       "0          naive       44333        1.000000       1.848534             0.0\n",
       "1          regex       34259        0.917866       1.177768             0.0\n",
       "2           nltk       34769        0.971512       2.211550             0.0\n",
       "3         razdel       34478        0.971297       1.786527             0.0\n",
       "4    nltk_porter       34750        0.971005       3.552431             0.0\n",
       "5  nltk_snowball       17434        0.859068       5.028917             0.0\n",
       "6          spacy       34695        0.970112      50.009183             0.0\n",
       "7      spacy_lem       19468        0.924172      97.268040             0.0\n",
       "8  nltk_pymorphy       17603        0.929640      18.611283             0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
    "\n",
    "texts = process_corpus(\"3_news_corpus_universal.jsonl\")\n",
    "print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\")\n",
    "\n",
    "results = run_experiment(texts)\n",
    "\n",
    "save_results(results)\n",
    "save_report(results)\n",
    "\n",
    "pd.DataFrame(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
